{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "News-Viralityipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvTWuZ_pcPfe",
        "colab_type": "code",
        "outputId": "b0ab7142-a81e-4517-eb13-add21df2152f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 927
        }
      },
      "source": [
        "!pip3 install newspaper3k datefinder"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting newspaper3k\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/b9/51afecb35bb61b188a4b44868001de348a0e8134b4dfa00ffc191567c4b9/newspaper3k-0.2.8-py3-none-any.whl (211kB)\n",
            "\r\u001b[K     |█▌                              | 10kB 30.3MB/s eta 0:00:01\r\u001b[K     |███                             | 20kB 22.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 30kB 28.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 40kB 32.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 51kB 23.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 61kB 26.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 71kB 22.0MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 81kB 23.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 92kB 25.1MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 102kB 23.0MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 112kB 23.0MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 122kB 23.0MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 133kB 23.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 143kB 23.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 153kB 23.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 163kB 23.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 174kB 23.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 184kB 23.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 194kB 23.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 204kB 23.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 215kB 23.0MB/s \n",
            "\u001b[?25hCollecting datefinder\n",
            "  Downloading https://files.pythonhosted.org/packages/16/2b/af8efaee30c0ba4238cb4d0645a07100d33d11d20a8783c443ed8b813eb9/datefinder-0.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.6.3)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.2.5)\n",
            "Collecting cssselect>=0.9.2\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.21.0)\n",
            "Collecting jieba3k>=0.35.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/cb/2c8332bcdc14d33b0bedd18ae0a4981a069c3513e445120da3c3f23a8aaa/jieba3k-0.35.1.zip (7.4MB)\n",
            "\u001b[K     |████████████████████████████████| 7.4MB 39.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (7.0.0)\n",
            "Collecting tinysegmenter==0.3\n",
            "  Downloading https://files.pythonhosted.org/packages/17/82/86982e4b6d16e4febc79c2a1d68ee3b707e8a020c5d2bc4af8052d0f136a/tinysegmenter-0.3.tar.gz\n",
            "Collecting feedparser>=5.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d8/7d37fec71ff7c9dbcdd80d2b48bcdd86d6af502156fc93846fb0102cb2c4/feedparser-5.2.1.tar.bz2 (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (2.8.1)\n",
            "Collecting tldextract>=2.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fd/0e/9ab599d6e78f0340bb1d1e28ddeacb38c8bb7f91a1b0eae9a24e9603782f/tldextract-2.2.2-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 8.4MB/s \n",
            "\u001b[?25hCollecting feedfinder2>=0.0.4\n",
            "  Downloading https://files.pythonhosted.org/packages/35/82/1251fefec3bb4b03fd966c7e7f7a41c9fc2bb00d823a34c13f847fd61406/feedfinder2-0.0.4.tar.gz\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (3.13)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from newspaper3k) (4.2.6)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from datefinder) (2018.9)\n",
            "Requirement already satisfied: regex>=2017.02.08 in /usr/local/lib/python3.6/dist-packages (from datefinder) (2019.12.20)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.2.1->newspaper3k) (1.12.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.10.0->newspaper3k) (2020.4.5.1)\n",
            "Collecting requests-file>=1.4\n",
            "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract>=2.0.1->newspaper3k) (46.1.3)\n",
            "Building wheels for collected packages: jieba3k, tinysegmenter, feedparser, feedfinder2\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-cp36-none-any.whl size=7398406 sha256=72b670686eba2d125246318056bee5b1e863654dd2d48b2f938c2c3490b31ade\n",
            "  Stored in directory: /root/.cache/pip/wheels/83/15/9c/a3f1f67e7f7181170ad37d32e503c35da20627c013f438ed34\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-cp36-none-any.whl size=13539 sha256=0ce84e816444903ed7a2bc8e10b3585ea1ce58738709335860a1414288eaa400\n",
            "  Stored in directory: /root/.cache/pip/wheels/81/2b/43/a02ede72324dd40cdd7ca53aad718c7710628e91b8b0dc0f02\n",
            "  Building wheel for feedparser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedparser: filename=feedparser-5.2.1-cp36-none-any.whl size=44940 sha256=484ff163d346ad35e2cf02e0f9b62876958079bcf0b8b8b6813c946f1929f60f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/69/b7/f52763c41c5471df57703a0ef718a32a5e81ee35dcf6d4f97f\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-cp36-none-any.whl size=3357 sha256=720a1ae5ed5d115d8f819a0fe939622ae39328bc8818b07003dba3e3d31d8ebe\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/03/ca/778e3a7a627e3d98836cc890e7cb40c7575424cfd3340f40ed\n",
            "Successfully built jieba3k tinysegmenter feedparser feedfinder2\n",
            "Installing collected packages: cssselect, jieba3k, tinysegmenter, feedparser, requests-file, tldextract, feedfinder2, newspaper3k, datefinder\n",
            "Successfully installed cssselect-1.1.0 datefinder-0.7.0 feedfinder2-0.0.4 feedparser-5.2.1 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 tinysegmenter-0.3 tldextract-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Neh8ROksuMEq",
        "colab_type": "code",
        "outputId": "979a9702-9a6c-4ffd-cb8f-c0036c512772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from newspaper import Article  \n",
        "import csv \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize \n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stopwords=set(stopwords.words('english'))\n",
        "from textblob import TextBlob\n",
        "import datefinder\n",
        "import datetime  \n",
        "from datetime import date \n",
        "from sklearn import metrics\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rvdh2GhJcNjU",
        "colab_type": "code",
        "outputId": "0bb377a8-d9db-47d7-de41-ec3825b688f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "url = \"https://timesofindia.indiatimes.com/world\"\n",
        "r = requests.get(url)\n",
        "soup = BeautifulSoup(r.content, 'html5lib') \n",
        "table = soup.findAll('a', attrs = {'class':'w_img'})\n",
        "\n",
        "news=[]\n",
        "for row in table: \n",
        "    if not row['href'].startswith('http'):\n",
        "        news.append('https://timesofindia.indiatimes.com'+row['href'])\n",
        "\n",
        "df=[]\n",
        "for i in news:\n",
        "    article = Article(i, language=\"en\")\n",
        "    article.download() \n",
        "    article.parse() \n",
        "    article.nlp() \n",
        "    data={}\n",
        "    data['Title']=article.title\n",
        "    data['Text']=article.text\n",
        "    data['Summary']=article.summary\n",
        "    data['Keywords']=article.keywords\n",
        "    df.append(data)\n",
        "\n",
        "\n",
        "dataset=pd.DataFrame(df)\n",
        "dataset.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Text</th>\n",
              "      <th>Summary</th>\n",
              "      <th>Keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Trump and Putin issue rare joint statement pro...</td>\n",
              "      <td>Reuters photo\\n\\nDownload The Times of India N...</td>\n",
              "      <td>Reuters photoDownload The Times of India News ...</td>\n",
              "      <td>[trump, moscow, officials, intelligence, state...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Covid-19 pandemic is far from over: WHO</td>\n",
              "      <td>Apr 28, 2020, 05:08PM IST\\n\\nSource: AP\\n\\nThe...</td>\n",
              "      <td>Apr 28, 2020, 05:08PM ISTSource: APThe head of...</td>\n",
              "      <td>[cases, virus, far, health, continue, pandemic...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Covid-19 in UK: We are beginning to turn the t...</td>\n",
              "      <td>Apr 28, 2020, 05:06PM IST\\n\\nSource: AP\\n\\nPri...</td>\n",
              "      <td>Apr 28, 2020, 05:06PM ISTSource: APPrime Minis...</td>\n",
              "      <td>[boris, hospital, work, country, understood, b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>US: First responders return to work after Covi...</td>\n",
              "      <td>Apr 28, 2020, 05:04PM IST\\n\\nSource: AP\\n\\nAcr...</td>\n",
              "      <td>Apr 28, 2020, 05:04PM ISTSource: APAcross the ...</td>\n",
              "      <td>[work, worries, responders, virus, puts, contr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Covid-19: Testing won't 'be a problem' for reo...</td>\n",
              "      <td>Apr 28, 2020, 08:40AM IST\\n\\nSource: AP\\n\\nThe...</td>\n",
              "      <td>Apr 28, 2020, 08:40AM ISTSource: APThe White H...</td>\n",
              "      <td>[trump, problem, white, tests, reopening, pres...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               Title  ...                                           Keywords\n",
              "0  Trump and Putin issue rare joint statement pro...  ...  [trump, moscow, officials, intelligence, state...\n",
              "1            Covid-19 pandemic is far from over: WHO  ...  [cases, virus, far, health, continue, pandemic...\n",
              "2  Covid-19 in UK: We are beginning to turn the t...  ...  [boris, hospital, work, country, understood, b...\n",
              "3  US: First responders return to work after Covi...  ...  [work, worries, responders, virus, puts, contr...\n",
              "4  Covid-19: Testing won't 'be a problem' for reo...  ...  [trump, problem, white, tests, reopening, pres...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xB2DCjGdtEb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rate_unique(words):\n",
        "    words=tokenize(words)\n",
        "    no_order = list(set(words))\n",
        "    rate_unique=len(no_order)/len(words)\n",
        "    return rate_unique\n",
        "\n",
        "def rate_nonstop(words):\n",
        "    words=tokenize(words)\n",
        "    filtered_sentence = [w for w in words if not w in stopwords]\n",
        "    rate_nonstop=len(filtered_sentence)/len(words)\n",
        "    no_order = list(set(filtered_sentence))\n",
        "    rate_unique_nonstop=len(no_order)/len(words)\n",
        "    return rate_nonstop,rate_unique_nonstop\n",
        "\n",
        "def avg_token(words):\n",
        "    words=tokenize(words)\n",
        "    length=[]\n",
        "    for i in words:\n",
        "        length.append(len(i))\n",
        "    return np.average(length)\n",
        "\n",
        "def day(article_text):\n",
        "    article=article_text\n",
        "    if len(list(datefinder.find_dates(article)))>0:\n",
        "        date=str(list(datefinder.find_dates(article))[0])\n",
        "        date=date.split()\n",
        "        date=date[0]\n",
        "        year, month, day = date.split('-')     \n",
        "        day_name = datetime.date(int(year), int(month), int(day)) \n",
        "        return day_name.strftime(\"%A\")\n",
        "    return \"Monday\"\n",
        "\n",
        "def tokenize(text):\n",
        "    text=text\n",
        "    return word_tokenize(text)\n",
        "\n",
        "pos_words=[]\n",
        "neg_words=[]\n",
        "def polar(words):\n",
        "    all_tokens=tokenize(words)\n",
        "    for i in all_tokens:\n",
        "        analysis=TextBlob(i)\n",
        "        polarity=analysis.sentiment.polarity\n",
        "        if polarity>0:\n",
        "            pos_words.append(i)\n",
        "        if polarity<0:\n",
        "            neg_words.append(i)\n",
        "    return pos_words,neg_words\n",
        "\n",
        "def rates(words):\n",
        "    words=polar(words)\n",
        "    pos=words[0]\n",
        "    neg=words[1]\n",
        "    all_words=words\n",
        "    global_rate_positive_words=(len(pos)/len(all_words))/100\n",
        "    global_rate_negative_words=(len(neg)/len(all_words))/100\n",
        "    pol_pos=[]\n",
        "    pol_neg=[]\n",
        "    for i in pos:\n",
        "        analysis=TextBlob(i)\n",
        "        pol_pos.append(analysis.sentiment.polarity)\n",
        "        avg_positive_polarity=analysis.sentiment.polarity\n",
        "    for j in neg:\n",
        "        analysis2=TextBlob(j)\n",
        "        pol_neg.append(analysis2.sentiment.polarity)\n",
        "        avg_negative_polarity=analysis2.sentiment.polarity\n",
        "    min_positive_polarity=min(pol_pos)\n",
        "    max_positive_polarity=max(pol_pos)\n",
        "    min_negative_polarity=min(pol_neg)\n",
        "    max_negative_polarity=max(pol_neg)\n",
        "    avg_positive_polarity=np.average(pol_pos)\n",
        "    avg_negative_polarity=np.average(pol_neg)\n",
        "    return global_rate_positive_words,global_rate_negative_words,avg_positive_polarity,min_positive_polarity,max_positive_polarity,avg_negative_polarity,min_negative_polarity,max_negative_polarity\n",
        "\n",
        "df2=[]\n",
        "for i in news:\n",
        "    pred_info={}\n",
        "    article = Article(i, language=\"en\") # en for English \n",
        "    article.download() \n",
        "    article.parse()\n",
        "    analysis=TextBlob(article.text)\n",
        "    polarity=analysis.sentiment.polarity\n",
        "    title_analysis=TextBlob(article.title)\n",
        "    pred_info['text']=article.text\n",
        "    pred_info['n_tokens_title']=len(tokenize(article.title))\n",
        "    pred_info['n_tokens_content']=len(tokenize(article.text))\n",
        "    pred_info['n_unique_tokens']=rate_unique(article.text)\n",
        "    pred_info['n_non_stop_words']=rate_nonstop(article.text)[0]\n",
        "    pred_info['n_non_stop_unique_tokens']=rate_nonstop(article.text)[1]\n",
        "    pred_info['num_hrefs']=article.html.count(\"https://timesofindia.indiatimes.com\")\n",
        "    pred_info['num_imgs']=len(article.images)\n",
        "    pred_info['num_videos']=len(article.movies)\n",
        "    pred_info['average_token_length']=avg_token(article.text)\n",
        "    pred_info['num_keywords']=len(article.keywords)\n",
        "    \n",
        "    if \"life-style\" in article.url:\n",
        "        pred_info['data_channel_is_lifestyle']=1\n",
        "    else:\n",
        "        pred_info['data_channel_is_lifestyle']=0\n",
        "    if \"etimes\" in article.url:\n",
        "        pred_info['data_channel_is_entertainment']=1\n",
        "    else:\n",
        "        pred_info['data_channel_is_entertainment']=0\n",
        "    if \"business\" in article.url:\n",
        "        pred_info['data_channel_is_bus']=1\n",
        "    else:\n",
        "        pred_info['data_channel_is_bus']=0\n",
        "    if \"social media\" or \"facebook\" or \"whatsapp\" in article.text.lower():\n",
        "        data_channel_is_socmed=1\n",
        "        data_channel_is_tech=0\n",
        "        data_channel_is_world=0\n",
        "    else:\n",
        "        data_channel_is_socmed=0\n",
        "    if (\"technology\" or \"tech\" in article.text.lower()) or (\"technology\" or \"tech\" in article.url):\n",
        "        data_channel_is_tech=1\n",
        "        data_channel_is_socmed=0\n",
        "        data_channel_is_world=0\n",
        "    else:\n",
        "        data_channel_is_tech=0\n",
        "    if \"world\" in article.url:\n",
        "        data_channel_is_world=1\n",
        "        data_channel_is_tech=0\n",
        "        data_channel_is_socmed=0\n",
        "    else:\n",
        "        data_channel_is_world=0\n",
        "        \n",
        "    pred_info['data_channel_is_socmed']=data_channel_is_socmed\n",
        "    pred_info['data_channel_is_tech']=data_channel_is_tech\n",
        "    pred_info['data_channel_is_world']=data_channel_is_world\n",
        "    \n",
        "    if day(i)==\"Monday\":\n",
        "        pred_info['weekday_is_monday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_monday']=0\n",
        "    if day(i)==\"Tuesday\":\n",
        "        pred_info['weekday_is_tuesday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_tuesday']=0\n",
        "    if day(i)==\"Wednesday\":\n",
        "        pred_info['weekday_is_wednesday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_wednesday']=0\n",
        "    if day(i)==\"Thursday\":\n",
        "        pred_info['weekday_is_thursday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_thursday']=0\n",
        "    if day(i)==\"Friday\":\n",
        "        pred_info['weekday_is_friday']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_friday']=0\n",
        "    if day(i)==\"Saturday\":\n",
        "        pred_info['weekday_is_saturday']=1\n",
        "        pred_info['is_weekend']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_saturday']=0\n",
        "    if day(i)==\"Sunday\":\n",
        "        pred_info['weekday_is_sunday']=1\n",
        "        pred_info['is_weekend']=1\n",
        "    else:\n",
        "        pred_info['weekday_is_sunday']=0\n",
        "        pred_info['is_weekend']=0\n",
        "        \n",
        "    pred_info['global_subjectivity']=analysis.sentiment.subjectivity\n",
        "    pred_info['global_sentiment_polarity']=analysis.sentiment.polarity\n",
        "    pred_info['global_rate_positive_words']=rates(article.text)[0]\n",
        "    pred_info['global_rate_negative_words']=rates(article.text)[1]\n",
        "    pred_info['avg_positive_polarity']=rates(article.text)[2]\n",
        "    pred_info['min_positive_polarity']=rates(article.text)[3]\n",
        "    pred_info['max_positive_polarity']=rates(article.text)[4]\n",
        "    pred_info['avg_negative_polarity']=rates(article.text)[5]\n",
        "    pred_info['min_negative_polarity']=rates(article.text)[6]\n",
        "    pred_info['max_negative_polarity']=rates(article.text)[7]    \n",
        "    pred_info['title_subjectivity']=title_analysis.sentiment.subjectivity\n",
        "    pred_info['title_sentiment_polarity']=title_analysis.sentiment.polarity\n",
        "    df2.append(pred_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MNxfXrcecJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "outputId": "337ec6a8-ec6b-4a5b-8a21-209c3565bed9"
      },
      "source": [
        "pred_df=pd.DataFrame(df2)\n",
        "pred_test=pred_df.drop(['text'],axis=1)\n",
        "pred_df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>n_tokens_title</th>\n",
              "      <th>n_tokens_content</th>\n",
              "      <th>n_unique_tokens</th>\n",
              "      <th>n_non_stop_words</th>\n",
              "      <th>n_non_stop_unique_tokens</th>\n",
              "      <th>num_hrefs</th>\n",
              "      <th>num_imgs</th>\n",
              "      <th>num_videos</th>\n",
              "      <th>average_token_length</th>\n",
              "      <th>num_keywords</th>\n",
              "      <th>data_channel_is_lifestyle</th>\n",
              "      <th>data_channel_is_entertainment</th>\n",
              "      <th>data_channel_is_bus</th>\n",
              "      <th>data_channel_is_socmed</th>\n",
              "      <th>data_channel_is_tech</th>\n",
              "      <th>data_channel_is_world</th>\n",
              "      <th>weekday_is_monday</th>\n",
              "      <th>weekday_is_tuesday</th>\n",
              "      <th>weekday_is_wednesday</th>\n",
              "      <th>weekday_is_thursday</th>\n",
              "      <th>weekday_is_friday</th>\n",
              "      <th>weekday_is_saturday</th>\n",
              "      <th>weekday_is_sunday</th>\n",
              "      <th>is_weekend</th>\n",
              "      <th>global_subjectivity</th>\n",
              "      <th>global_sentiment_polarity</th>\n",
              "      <th>global_rate_positive_words</th>\n",
              "      <th>global_rate_negative_words</th>\n",
              "      <th>avg_positive_polarity</th>\n",
              "      <th>min_positive_polarity</th>\n",
              "      <th>max_positive_polarity</th>\n",
              "      <th>avg_negative_polarity</th>\n",
              "      <th>min_negative_polarity</th>\n",
              "      <th>max_negative_polarity</th>\n",
              "      <th>title_subjectivity</th>\n",
              "      <th>title_sentiment_polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reuters photo\\n\\nDownload The Times of India N...</td>\n",
              "      <td>9</td>\n",
              "      <td>400</td>\n",
              "      <td>0.555000</td>\n",
              "      <td>0.667500</td>\n",
              "      <td>0.467500</td>\n",
              "      <td>231</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>5.052500</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.404762</td>\n",
              "      <td>0.078571</td>\n",
              "      <td>0.025</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>0.900000</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Apr 28, 2020, 05:08PM IST\\n\\nSource: AP\\n\\nThe...</td>\n",
              "      <td>8</td>\n",
              "      <td>253</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.691700</td>\n",
              "      <td>0.490119</td>\n",
              "      <td>185</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4.351779</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.551094</td>\n",
              "      <td>0.132183</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.288320</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.648980</td>\n",
              "      <td>-0.714286</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Apr 28, 2020, 05:06PM IST\\n\\nSource: AP\\n\\nPri...</td>\n",
              "      <td>15</td>\n",
              "      <td>229</td>\n",
              "      <td>0.663755</td>\n",
              "      <td>0.655022</td>\n",
              "      <td>0.515284</td>\n",
              "      <td>185</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4.305677</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.226105</td>\n",
              "      <td>-0.063520</td>\n",
              "      <td>0.735</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.260606</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.422826</td>\n",
              "      <td>-0.714286</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Apr 28, 2020, 05:04PM IST\\n\\nSource: AP\\n\\nAcr...</td>\n",
              "      <td>9</td>\n",
              "      <td>323</td>\n",
              "      <td>0.640867</td>\n",
              "      <td>0.650155</td>\n",
              "      <td>0.482972</td>\n",
              "      <td>185</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4.269350</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.342922</td>\n",
              "      <td>0.081739</td>\n",
              "      <td>0.890</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.263076</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.5</td>\n",
              "      <td>-0.355371</td>\n",
              "      <td>-0.714286</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Apr 28, 2020, 08:40AM IST\\n\\nSource: AP\\n\\nThe...</td>\n",
              "      <td>15</td>\n",
              "      <td>152</td>\n",
              "      <td>0.736842</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.539474</td>\n",
              "      <td>185</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>4.486842</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0.342472</td>\n",
              "      <td>0.174148</td>\n",
              "      <td>1.280</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.283119</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.7</td>\n",
              "      <td>-0.330306</td>\n",
              "      <td>-0.714286</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...  title_sentiment_polarity\n",
              "0  Reuters photo\\n\\nDownload The Times of India N...  ...                      0.30\n",
              "1  Apr 28, 2020, 05:08PM IST\\n\\nSource: AP\\n\\nThe...  ...                      0.10\n",
              "2  Apr 28, 2020, 05:06PM IST\\n\\nSource: AP\\n\\nPri...  ...                      0.00\n",
              "3  Apr 28, 2020, 05:04PM IST\\n\\nSource: AP\\n\\nAcr...  ...                      0.25\n",
              "4  Apr 28, 2020, 08:40AM IST\\n\\nSource: AP\\n\\nThe...  ...                      0.00\n",
              "\n",
              "[5 rows x 37 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVdcLj0DqxpR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1-vS-A-eh_v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_cols(data):\n",
        "    clean_col_map = {x: x.lower().strip() for x in list(data)}\n",
        "    return data.rename(index=str, columns=clean_col_map)\n",
        "\n",
        "def TrainTestSplit(X, Y, R=0, test_size=0.2):\n",
        "    return train_test_split(X, Y, test_size=test_size, random_state=R)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbJzCGSHrFa9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "col = list(['n_tokens_title', 'n_tokens_content', 'n_unique_tokens',\n",
        "       'n_non_stop_words', 'n_non_stop_unique_tokens', 'num_hrefs', 'num_imgs',\n",
        "       'num_videos', 'average_token_length', 'num_keywords',\n",
        "       'data_channel_is_lifestyle', 'data_channel_is_entertainment',\n",
        "       'data_channel_is_bus', 'data_channel_is_socmed', 'data_channel_is_tech',\n",
        "       'data_channel_is_world', 'weekday_is_monday', 'weekday_is_tuesday',\n",
        "       'weekday_is_wednesday', 'weekday_is_thursday', 'weekday_is_friday',\n",
        "       'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend',\n",
        "       'global_subjectivity', 'global_sentiment_polarity',\n",
        "       'global_rate_positive_words', 'global_rate_negative_words',\n",
        "       'avg_positive_polarity', 'min_positive_polarity',\n",
        "       'max_positive_polarity', 'avg_negative_polarity',\n",
        "       'min_negative_polarity', 'max_negative_polarity', 'title_subjectivity',\n",
        "       'title_sentiment_polarity','shares'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKAXs-HIt-G4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file = pd.read_csv('/content/drive/My Drive/data.csv')\n",
        "full_data = clean_cols(file)\n",
        "full_data = full_data[col]\n",
        "X = full_data[col[0:-1]]\n",
        "y = full_data[col[-1]]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiSy0x8K3nzh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "97f8919f-b13b-4002-e62f-66d6311f844d"
      },
      "source": [
        "rfr = RandomForestRegressor(min_samples_split=9)\n",
        "rfr.fit(X_train, y_train)\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=9, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=100, n_jobs=None, oob_score=False,\n",
              "                      random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7I402spss8I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1121b1d9-66d6-45a0-df3d-8dee5cde158f"
      },
      "source": [
        "predicted_test = rf.predict(X_test)\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predicted_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Absolute Error: 3502.252798694958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayW5f-8XvSRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from xgboost import XGBRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGTwwcYt7fJK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "78483219-2b45-4052-cbea-ae085b080aef"
      },
      "source": [
        "rf = XGBRegressor(n_estimators = 1000, random_state = 0 , max_depth = 15)\n",
        "rf.fit(X_train,y_train)\n",
        "predicted_test = rf.predict(X_test)\n",
        "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, predicted_test))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[18:06:20] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "Mean Absolute Error: 3627.237236903693\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "553ayfF7-wt4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "outputId": "5b49f655-9275-46ba-f527-8e5ffa9a2269"
      },
      "source": [
        "pred_test=pd.DataFrame(rfr.predict(X_test))\n",
        "pred_test.reset_index(level=0, inplace=True)\n",
        "pred_test = pred_test.rename(index=str, columns={\"index\": \"News\", 0: \"Virality\"})\n",
        "pred_test.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>News</th>\n",
              "      <th>Virality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3695.243068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2625.312128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>7154.539147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>6015.816544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2631.503608</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   News     Virality\n",
              "0     0  3695.243068\n",
              "1     1  2625.312128\n",
              "2     2  7154.539147\n",
              "3     3  6015.816544\n",
              "4     4  2631.503608"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7j0N0xF7pW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}